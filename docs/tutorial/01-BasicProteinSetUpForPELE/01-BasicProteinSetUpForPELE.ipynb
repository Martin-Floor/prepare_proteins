{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4460f729",
   "metadata": {},
   "source": [
    "# 01 - Preparing your proteins\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The prepare_proteins library was written to deal with the high throughput setup of protein systems. It can handle many PDB files simultaneously to set up general optimizations that prepare the systems for specific calculations and simulations.\n",
    "\n",
    "This document will show an example of the general workflow that can be followed to accomplish the previously mentioned objectives. We will work with several glutathione peroxidases (GPX) sequences from building their models (with Alpha Fold) to setting up PELE simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffd7c0",
   "metadata": {},
   "source": [
    "## 1. What modules and libraries do we need?\n",
    "\n",
    "First, we need to import the main library **\"prepare_proteins\"**. \n",
    "\n",
    "Second, we will also import an additional library to help us send calculations to the different BSC clusters. The **\"bsc_calculations\"** library sets up the calculation files, folders and slurm scripts for efficiently launching jobs to the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_proteins\n",
    "import bsc_calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818f294",
   "metadata": {},
   "source": [
    "We will also load other common Python libraries to help us in out set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c722cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd988de",
   "metadata": {},
   "source": [
    "## 2. Preparing sequences - starting from a FASTA file\n",
    "\n",
    "In this case, we are starting from protein sequences, so we need to model their protein structures. We will set up AlphaFold calculations from a FASTA file (\"gpx_sequences.fasta\") containing five GPX sequences. \n",
    "\n",
    "The first step is to initialise the *sequenceModels* class with the path to our fasta file. We assigned the initialised class to the variable *sequences*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6641d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = prepare_proteins.sequenceModels('gpx_sequences.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aae382",
   "metadata": {},
   "source": [
    "Now we can use the class method *setUpAlphafold* to create all the files, folders and commands to launch AlphaFold. It takes as the only parameter the folder's name in which we want to put our calculation files. The method returns a list of the commands that must be executed to run the job. We store that list in a variable called jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = sequences.setUpAlphaFold('alphafold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d619d3",
   "metadata": {},
   "source": [
    "Finally, we can create a slurm script to launch the AlphaFold jobs using the **\"bsc_calculations\"** library. Since the job will be run in the Minotauro cluster, we call a method inside the corresponding sub-class from the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_calculations.minotauro.jobArrays(jobs, job_name='AF_sequences', partition='bsc_ls', program='alphafold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2044f6e",
   "metadata": {},
   "source": [
    "The *jobArrays* method needs the list of commands to generate the slurm script file. We have specified the 'bsc_ls' partition to run the calculations, and with the keyword \"program\", we tell the script to load all necessary libraries to run AlphaFold in this cluster.\n",
    "\n",
    "To launch the calculations, you will need to upload the 'AF_sequences' folder and the 'slurm_array.sh' script to the cluster and then launch it with: \n",
    "\n",
    "    sbatch slurm_array.sh\n",
    "\n",
    "After all the AlphaFold calculation has finished, we will need to get the protein structures output from the cluster. Since AlphaFold generates large-memory outputs, we are only interested in grabbing the PDB files to load them into our library. This can be easily done with a command like this:\n",
    "\n",
    "    tar cvf AF_sequences.tar AF_sequences/output_models/\\*/relaxed_model_\\*pdb\"\n",
    "\n",
    "The tar file contains only the relaxed PDB outputs but maintains the folder structure of our AlphaFold calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df88d0",
   "metadata": {},
   "source": [
    "### 2.2. Preparing models - taking PDB files\n",
    "\n",
    "After getting our AlphaFold results from the cluster, we need to put them into a folder renamed with their corresponding protein names. To do that, we run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structures folder if it does not exists\n",
    "if not os.path.exists('structures'):\n",
    "    os.mkdir('structures')\n",
    "    \n",
    "# Copy each alphafold output model (from a specific rank) into the structures folder\n",
    "rank = 0\n",
    "for model in os.listdir('alphafold/output_models/'):\n",
    "    if os.path.exists('alphafold/output_models/'+model+'/ranked_'+str(rank)+'.pdb'):\n",
    "        shutil.copyfile('alphafold/output_models/'+model+'/ranked_'+str(rank)+'.pdb', \n",
    "                        'structures/'+model+'.pdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff09b2",
   "metadata": {},
   "source": [
    "Now we can initialise the *proteinModels* class with our PDB files from the structures folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = prepare_proteins.proteinModels('structures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98755dde",
   "metadata": {},
   "source": [
    "The library reads all PDB files as [biopython Bio.PDB.Structure()](https://biopython.org/wiki/The_Biopython_Structural_Bioinformatics_FAQ) objects at the structures attribute. This attribute is a dictionary whose keys are the protein models' names, and the values are the Bio.PDB objects. The library can be iterated to get the protein models' names at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6956821",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    print(models.structures[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56220448",
   "metadata": {},
   "source": [
    "## 3. System preparation\n",
    "\n",
    "### 3.1 Removing low confidence regions from AlphaFold models at the protein termini\n",
    "\n",
    "AlphaFold models can contain structural regions with low confidence in their prediction. Since this can represent large structural domains or segments, we are interested in removing them, mainly if they are found at the N- and C-termini.\n",
    "\n",
    "The **prepapare_proteins** library has a method to remove terminal segments from AlphaFold structures using the confidence score stored at the B-factor column of the PDBs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852eb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.removeTerminiByConfidenceScore(confidence_threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80ae9e",
   "metadata": {},
   "source": [
    "The condifedence_threshold keyword indicates the maximum confidence score at which to stop the trimming of terminal regions. \n",
    "\n",
    "When we are modifying our proteins, it is good to check that the structural changes have been carried out as expected. The library has a method for writing all the structures into a folder so we can visualise the state of our set-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020192e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.saveModels('trimmed_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717353a9",
   "metadata": {},
   "source": [
    "We can open the PDB files with any external programs to check what the previous code did.\n",
    "\n",
    "In the current state of the library, after some modifications to the structures, we need to re-initialise the *proteinModels* class using the models written to a folder with the saveModels() method. For this, we repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = prepare_proteins.proteinModels('trimmed_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029adb0",
   "metadata": {},
   "source": [
    "### 3.2 Align structures to a reference PDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae662aa7",
   "metadata": {},
   "source": [
    "When comparing related proteins, it is good to align them to have a common structural framework. The library helps you align the proteins with the method alignModelsToReferencePDB(). We give a reference PDB (any PDB from our models would do), then a folder where to write the aligned structures and the index (or indexes) of the chains to align (see the documentation inside the function for details on how the chain_indexes are given). For now, we set up the index to be the first folder in the structure (chain_indexes=0), and we run the alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9783fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.alignModelsToReferencePDB('trimmed_models/GPX_Bacillus-subtilis.pdb', 'aligned_models', chain_indexes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eb989d",
   "metadata": {},
   "source": [
    "We will continue working with the aligned structures; for that, we reload our output models into the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c0d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = prepare_proteins.proteinModels('aligned_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c28f26",
   "metadata": {},
   "source": [
    "# 4. Prepwizard optimizations\n",
    "\n",
    "After our protein models are correctly trimmed and aligned, we can continue with the Prepwizard optimization of the structures. We create this set-up by calling the method setUpPrepwizardOptimization():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1efcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = models.setUpPrepwizardOptimization('prepwizard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de04b86d",
   "metadata": {},
   "source": [
    "Again, the method needs a folder name to put all input files for the calculations. After executing the method, it returns the commands to be executed for running the Prepwizard optimization in a machine endowed with the Schrodinger Software license. The commands can be passed to the **bsc_calculations** library to generate the scripts to facilitate the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d91550",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_calculations.local.parallel(jobs, cpus=min([40, len(jobs)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8cc78",
   "metadata": {},
   "source": [
    "We pass the commands to the local sub-class and define the number of CPUs we want to use beforehand, so the library will create one script for each CPU to be used. In our case, we are working with five files, so only five script files were created. The script files are named commands_?; where \"?\" stands for an integer identifying the individual script. The method also writes a script called commands, which can launch all the scripts simultaneously. So, to run the optimizations, we need to upload the 'prepwizard' folder to a cluster with a Schrodinger license and all the commands scripts. Then to launch the calculation, you need to run:\n",
    "\n",
    "    bash commands\n",
    "    \n",
    "The execution can be followed by looking at the log files being generated at the output_models folder inside the 'prepwizard' job folder. \n",
    "\n",
    "After all the Prepwizard optimization jobs are over, we need to download them into the folder containing this notebook and load them into the library with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.loadModelsFromPrepwizardFolder('prepwizard/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07593256",
   "metadata": {},
   "source": [
    "If any model is missing from the optimization, the library will issue a warning specifying which models were not found. If that's the case, we suggest looking at the log files to check what could have been wrong. \n",
    "\n",
    "After loading the optimization models into the library, it is again recommended to check them visually. For that, we save them into a 'prepwizard_models' folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e9cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.saveModels('prepwizard_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a826b11",
   "metadata": {},
   "source": [
    "# 5. Grid set up for Glide docking\n",
    "\n",
    "We need to generate starting ligand conformations for our systems to run PELE. We can achieve this by running docking simulations with Glide docking. The first step to achieve this is to generate grid files suitable for protein-ligand docking calculations using the Glide docking engine provided by Schrodinger.\n",
    "\n",
    "## 5.1 Defining residues for the centre of the docking calculations\n",
    "\n",
    "To set the grid calculations, we first need to define the atoms serving as the centre for our docking calculations. This can get complicated when working with multiple proteins since the residues have different PDB indexes. To identify equivalent residues in an evolutionary framework of related proteins, we employ multiple sequence alignments (MSAs) of all the loaded protein models. The library can quickly run an MSA calculation using the mafft (command-line) program. The program is called internally, so we only need to execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd0a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msa = models.calculateMSA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be36fac",
   "metadata": {},
   "source": [
    "The 'msa' object is read into the framework of [Biopython's Bio.Align.MultipleSeqAlignment](https://biopython-tutorial.readthedocs.io/en/latest/notebooks/06%20-%20Multiple%20Sequence%20Alignment%20objects.html) object. We can write this multiple sequence alignment to a file to check it with an external program (we recommend using chimera because it can easily map the sequence to the structures, but any would do for a general-purpose):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f981f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_proteins.alignment.writeMsaToFastaFile(msa, 'msa.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebc5e8",
   "metadata": {},
   "source": [
    "In our case, we are interested in a specific cysteine residue conserved in all the sequences. To speed up the search of the MSA position corresponding to this conserved residue, we can use a method to find all conserved MSA positions and their identities. Since this tutorial uses very few protein models, there are several conserved residues; however, the conservation becomes more meaningful when a high number of proteins are compared. For our case, we will print only the Cysteine residues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3cb900",
   "metadata": {},
   "outputs": [],
   "source": [
    "conserved_index = models.getConservedMSAPositions(msa)\n",
    "for c in conserved_index:\n",
    "    if c[1] == 'C':\n",
    "        print(c)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b515c",
   "metadata": {},
   "source": [
    "Two conserved residues are present; a quick search of the literature defines the conserved Cysteine residue as the one with MSA index 33. We employ this MSA index to get the PDB indexes for all the proteins loaded into our library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762fee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "cys_index = models.getStructurePositionFromMSAindex(33)\n",
    "print(cys_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf23c7",
   "metadata": {},
   "source": [
    "We can continue to define the centre of our grid calculation with those indexes.\n",
    "\n",
    "## 5.2 Set up the grid calculation\n",
    "\n",
    "We define the centre of the grid as the 'SG' atom of the Cysteine residues previously defined. We build a dictionary representing the atoms as 3-elements tuples: (chain_id, residue_id, atom_name). To grab all this information is safest if we employ the Bio.PDB.Structure objects inside the library. We are going to iterate these objects, searching for the residues that match our indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9655c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cys_center_atom = {} # Create dictionary to store the atom 3-element tuple for each model\n",
    "for model in models: # Iterate the models inside the library\n",
    "    for r in models.structures[model].get_residues(): # Iterate the residues for each Bio.PDB.Structure object\n",
    "        if r.id[1] == cys_index[model]: # Check that the residue matches the defined index\n",
    "            assert r.resname == 'CYS' # Assert that the residue has the correct residue identity\n",
    "            cys_center_atom[model] = (r.get_parent().id, r.id[1], 'SG') # Store the corresponsing tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd392c",
   "metadata": {},
   "source": [
    "Now that we have our center atoms we proceed to set up the grid calculations folder and script, analogously to what we did with the Prepwizard optimization step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = models.setUpDockingGrid('grid', cys_center_atom) # Set grid calcualtion\n",
    "bsc_calculations.local.parallel(jobs, cpus=min([40, len(jobs)])) # Write the scripts to run them locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c63f04",
   "metadata": {},
   "source": [
    "The setUpDockingGrid() method needs the folder name where to put the calculation's files and folders. The second argument is the dictionary with the centre atoms (one per model in the library).\n",
    "\n",
    "We also create the scripts for running the grid calculation (note that this will overwrite the previous scripts written down for the Prepwizard calculations, so it is always a good idea to check these scripts before running them).\n",
    "\n",
    "The launching of the calculation is very similar to the Prepwizard optimization case so you can check above for details.\n",
    "\n",
    "After the calculation has finished, we need to download the results. However, it is recommended not to delete the outputs from the cluster where they were run; we still need them for the Glide Docking job.\n",
    "\n",
    "# 6. Set up Glide docking\n",
    "\n",
    "Running Glide Docking in our proteins will need a set of ligands to be docked. It is a usual practice to draw them in the free Maestro 2D sketcher and store them as .mae files in a common folder. However, here, we have downloaded the ligand directly from the PDB database, and therefore it will need to be converted to the .mae format. This task can become tediously manual if the number of ligands is vast, this is the reason why we have implemented a method for converting all ligand PDB files inside a specific folder into the .mae format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0300ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models.convertLigandPDBtoMae('ligands')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10942ab",
   "metadata": {},
   "source": [
    "After running the function, the folder will contain the .mae files inside it and now can be given to the docking set-up method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = models.setUpGlideDocking('docking', 'grid', 'ligands', poses_per_lig=100)\n",
    "bsc_calculations.local.parallel(jobs, cpus=min([40, len(jobs)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1f8d7",
   "metadata": {},
   "source": [
    "The setUpGlideDocking() function needs three mandatory arguments, the 'docking' folder, where the docking job will be stored; the 'grid' folder, where the grid calculation outputs are located; and the 'ligands' folder with the ligand structures in .mae format. We have specified the number of docking trajectories with the keyword poses_per_lig.\n",
    "\n",
    "After running and downloading the docking results, they can be analysed to extract the best poses to feed them to the PELE set-up.\n",
    "\n",
    "# 7. Analyse docking calculations\n",
    "\n",
    "## 7.1 Calculating docking distances\n",
    "\n",
    "The best docking structures should be selected according to their Glide Score; however, on many occasions, we also would like to filter the poses by different protein-ligand distances that represent the best conformations according to what we are aiming to simulate. For this reason, we will do the docking analysis by calculating the distance between the ligand sulfur atom and the SG atom of the catalytic cysteine of the GPX enzymes.\n",
    "\n",
    "First, we build a dictionary containing the docking distances we want to calculate from the conformations genereated from the Glide docking calculation. This dictionary (atom_pairs) is a doubly nested dictionary that should go first for all protein models and then for all the ligands that were docked:\n",
    "\n",
    "atom_pairs = {\\\n",
    "$\\;\\;\\;\\;$model1 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$ligand1 : [(protin_atom1, ligand_atom1), (protin_atom1, ligand_atom1), etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$ligand2 : [(protin_atom1, ligand_atom1), (protin_atom1, ligand_atom1), etc...]},\\\n",
    "$\\;\\;\\;\\;$model2 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$ligand1 : [(protin_atom1, ligand_atom1), (protin_atom1, ligand_atom1), etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$ligand2 : [(protin_atom1, ligand_atom1), (protin_atom1, ligand_atom1), etc...]},\\\n",
    "$\\;\\;\\;\\;$etc...\\\n",
    "}\n",
    "\n",
    "Each doubly nested dictionary, representing the analysis for each protein and ligand, should contain a list of the atom pairs to use in the distance calculation. Similarly, as done for the atom_centers, each atom must be represented as a 3-element tuple (see above) or, in the case of the ligand atom, just with a string representing the ligand atom name.\n",
    "\n",
    "We build the dictionary by iterating the models and by combining the ligand sulphur atom name (S1) with Cysteine residues previously used as atom centres of the grid calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_pairs = {} # Define the dictionary containing the atom pairs for each model\n",
    "for model in models:\n",
    "    atom_pairs[model] = {} \n",
    "    for ligand in ['GSH']:\n",
    "        atom_pairs[model][ligand] = []\n",
    "        atom_pairs[model][ligand].append((cys_center_atom[model], 'S1'))\n",
    "atom_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8adeb0d",
   "metadata": {},
   "source": [
    "We can now use this nested dictionary as input for our docking analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafad6ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models.analyseDocking('docking', atom_pairs=atom_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca7593",
   "metadata": {},
   "source": [
    "The docking analysis data is stored as a [panda dataframe](https://pandas.pydata.org/) in the attribute .docking_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba68aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.docking_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c0693",
   "metadata": {},
   "source": [
    "## 7.2 Selecting the best poses according to a common metric\n",
    "\n",
    "To facilitate pose selection according to a common distance, we first need to group the distances under a common name (from now on metric) in our data frame. We can use the method combineDockingDistancesIntoMetrics(), which will gather all distances in a list and combine them by taking the minimum of the distance values. To use the function, we (again) need to construct a triply nested dictionary which goes from each metric, model, and ligand and contains as values the lists (of each model + ligand), which will be combined under the common metric name:\n",
    "\n",
    "metric_distances = {\\\n",
    "$\\;\\;\\;\\;$metric_label_1 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$model1 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand1 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand2 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$model2 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand1 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand2 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$etc...\\\n",
    "$\\;\\;\\;\\;$metric_label_2 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$model1 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand1 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand2 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$model2 : {\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand1 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ligand2 : [(distance_label_1, distance_label_2, etc...],\\\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;$etc...\\\n",
    "$\\;\\;\\;\\;$etc...\\\n",
    "}\n",
    "\n",
    "The construction of this dictionary is facilitated by employing the method getDockingDistances(), which will return the list of distance labels associated with a specific protein + ligand docking. Since all distances calculated in this example will be stored under the same metric, we can straightforwardly build the 'metric_distances' dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c113ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_distances = {} # Define the global dictionary\n",
    "metric_distances['SG_S'] = {} # Define the metric nested dictionary\n",
    "for model in models:\n",
    "    metric_distances['SG_S'][model] = {} # Define the model nested dictionary\n",
    "    for ligand in models.docking_ligands[model]: \n",
    "        # Define the ligand nested dictionary with all the docking distances list\n",
    "        metric_distances['SG_S'][model][ligand] = models.getDockingDistances(model, ligand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1756b4a6",
   "metadata": {},
   "source": [
    "We now give this dictionary to the combineDockingDistancesIntoMetrics() method and inspect the changes upon our docking data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.combineDockingDistancesIntoMetrics(metric_distances)\n",
    "models.docking_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421d80ef",
   "metadata": {},
   "source": [
    "Now that our docking data frame contains a new column with the docking distances grouped under the same name, we can use this information to extract the best poses resulting from each docking run. \n",
    "\n",
    "This can be achieved by giving a threshold value to obtain all catalytic poses fulfilling a specific value for the common metric and getting the ones with the best Glide score. However, it is not always the case that all docking runs (i.e., for each protein and ligand combination) produce reasonable distances. In this case, many docking results would be left out if the distance thresholds employed were too restrictive. A way to solvent this would be to select a small distance threshold, choose the best models that fulfilled it, and then increase this threshold in a small amount to gather the best models containing slightly large metric distances. If we repeat this iteratively, we could get the best possible models for all the docking runs without compromising our choice by using a larger threshold for poses with already good metric values. \n",
    "\n",
    "Luckily, this process has been automated in the library so that the docking selection can be carried out easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_poses = models.getBestDockingPosesIteratively(metric_distances, max_threshold=7)\n",
    "best_poses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663d53aa",
   "metadata": {},
   "source": [
    "To use the function, we only need to give the name of the metrics that will be employed in the filtering.* The function defines the iteration process with the following keywords: \n",
    "\n",
    "min_threshold=3.5\\\n",
    "max_threshold=5.0\\\n",
    "step_size=0.1\n",
    "\n",
    "This is similar to the [np.arange()](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) function works, and it can be tweaked to obtain the optimal results for the selection you are aiming to get.\n",
    "\n",
    "The best poses thus selected are returned as a pandas data frame and are used to set up the final PELE calculation.\n",
    "\n",
    "*(Caution: in the current implementation of the function, the same threshold value is employed at each iteration for all metric values).\n",
    "\n",
    "## 7.3 Extracting docking poses\n",
    "\n",
    "We can extract a subset of docking poses by giving a filtered pandas data frame to the method extractDockingPoses(). \n",
    "In our case, we use the 'best_poses' data frame containing one pose per docking run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.extractDockingPoses(best_poses, 'docking', 'best_poses', separator='@')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f698c",
   "metadata": {},
   "source": [
    "The inputs are the filtered data frame, the 'docking' folder, and the folder where to store the poses. Files' names will be given as a combination between the model_name+ligand+docking_pose and separated with the symbol provided at the 'separator' keyword. A warning will be raised if the protein or ligand name contains the separator in their name.\n",
    "\n",
    "# 8. Setting up PELE\n",
    "\n",
    "Once the docking poses are extracted, the final step is setting up the PELE calculation folders, files, and slurm scripts. The method needs the folder where to store the calculation information ('pele'), the folder containing the docking poses ('best_poses'), and an input YAML file containing the details of the PELE platform protocol. We also give our atom_pairs dictionary to calculate the same distances employed in our docking analysis, so they are calculated throughout the PELE simulation. \n",
    "\n",
    "Since the ligand index in our docking poses is zero, we change the default ligand index (1) with the keyword  ligand_index. Finally, since we change the separator when writing the docking poses, we need to make the setUpPELECalculation() method aware using the 'separator' keyword again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = models.setUpPELECalculation('pele', 'best_poses/', 'input.yaml', \n",
    "                                   distances=atom_pairs, ligand_index=0, separator='@')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6bcf0",
   "metadata": {},
   "source": [
    "The commands generated by this method are slightly different than in other functions. However, they can be quickly processed by employing the setUpPELEForMarenostrum() method inside the bsc_calculations library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5960c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_calculations.marenostrum.setUpPELEForMarenostrum(jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
