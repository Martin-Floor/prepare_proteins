{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a63a02",
   "metadata": {},
   "source": [
    "docu:\n",
    " - sequenceModels\n",
    " - setUpAlphaFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caaf128",
   "metadata": {},
   "source": [
    "## * General considerations\n",
    "#### Get the information of a method / function\n",
    "We can always see information of a method, for example a function, by writting a \"?\" at the end of the line. It will retrieve the function's documentation, typically a general explanation of what does the function do and its options. In the example below, we are going to take the \"models\" variable where we used the proteinModels class and stablished our \"models_folder\" and we will apply the *removeTerminiByConfidenceScore* function. To see its documentation, we add a \"?\" at the end of the function's name and execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f8b954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `models.removeTerminiByConfidenceScore` not found.\n"
     ]
    }
   ],
   "source": [
    "models.removeTerminiByConfidenceScore?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460f729",
   "metadata": {},
   "source": [
    "# 01 - Preparing your proteins\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The prepare_proteins library was written to deal with the high throughput setup of protein systems. It can handle many PDB files simultaneously to set up general optimizations that prepare the systems for specific calculations and simulations.\n",
    "\n",
    "In this document, we will show an example of the general workflow that can be followed to accomplish the previously mentioned objectives. We will work with several glutathione peroxidase (GPX) sequences from building their models (with Alpha Fold) to setting up PELE simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffd7c0",
   "metadata": {},
   "source": [
    "## 1. What modules and libraries do we need?\n",
    "\n",
    "First, we need to import the main library **\"prepare_proteins\"**. \n",
    "\n",
    "Second, we will also import an additional library to help us send calculations to the different BSC clusters. The **\"bsc_calculations\"** library sets up the calculation files, folders and slurm scripts for efficiently launching jobs to the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149b2778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prepare_proteins\n",
    "import bsc_calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818f294",
   "metadata": {},
   "source": [
    "We will also load other common Python libraries to help us in out set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c722cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd988de",
   "metadata": {},
   "source": [
    "## 2. Preparing sequences - starting from a FASTA file\n",
    "\n",
    "In this case, we are starting from protein sequences, so we need to model their protein structures. We will set up AlphaFold calculations from a FASTA file (\"gpx_sequences.fasta\") containing five GPX sequences. \n",
    "\n",
    "The first step is to initialise the *sequenceModels* class with the path to our fasta file. We assigned the initialised class to the variable *sequences*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6641d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = prepare_proteins.sequenceModels('gpx_sequences.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aae382",
   "metadata": {},
   "source": [
    "Now we can use the class method *setUpAlphafold* to create all the files, folders and commands to launch AlphaFold. It takes as the only parameter the folder's name in which we want to put our calculation files. The method returns a list of the commands that must be executed to run the job. We store that list in a variable called jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0866149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = sequences.setUpAlphaFold('alphafold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d619d3",
   "metadata": {},
   "source": [
    "Finally, we can create a slurm script to launch the AlphaFold jobs using the **\"bsc_calculations\"** library. Since the job will be run in the Minotauro cluster, we call a method inside the corresponding sub-class from the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d4ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_calculations.minotauro.jobArrays(jobs, job_name='AF_sequences', partition='bsc_ls', program='alphafold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2044f6e",
   "metadata": {},
   "source": [
    "The *jobArrays* method needs the list of commands to generate the slurm script file. We have specified the 'bsc_ls' partition to run the calculations, and with the keyword \"program\", we tell the script to load all necessary libraries to run AlphaFold in this cluster.\n",
    "\n",
    "To launch the calculations you will need to upload the 'AF_sequences' folder and the 'slurm_array.sh' script to the cluster and then launch it with: \n",
    "\n",
    "    sbatch slurm_array.sh\n",
    "\n",
    "After all the AlphaFold calculation has finished, we will need to get the protein structures output from the cluster. Since AlphaFold generates large-memory outputs we are only interested in grabbing the PDB files to load them into our library. This can be easily done with a command like this:\n",
    "\n",
    "    tar cvf AF_sequences.tar AF_sequences/output_models/\\*/relaxed_model_\\*pdb\"\n",
    "\n",
    "The tar file contains only the relaxed PDB outputs but mantains the folder structure of our AlphaFold calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df88d0",
   "metadata": {},
   "source": [
    "### 2.2. Preparing models - taking PDB files\n",
    "\n",
    "After we get our AlphaFold results from the cluster we need to put them into a folder renamed woth their corrsponfing protein names. To do that we run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d195b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structures folder if it does not exists\n",
    "if not os.path.exists('structures'):\n",
    "    os.mkdir('structures')\n",
    "    \n",
    "# Copy each alphafold output model (from a specific rank) into the structures folder\n",
    "rank = 0\n",
    "for model in os.listdir('alphafold/output_models/'):\n",
    "    if os.path.exists('alphafold/output_models/'+model+'/ranked_'+str(rank)+'.pdb'):\n",
    "        shutil.copyfile('alphafold/output_models/'+model+'/ranked_'+str(rank)+'.pdb', \n",
    "                        'structures/'+model+'.pdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ff09b2",
   "metadata": {},
   "source": [
    "Now we can initialise the *proteinModels* class with our PDB files from the structures folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3619fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = prepare_proteins.proteinModels('structures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98755dde",
   "metadata": {},
   "source": [
    "The library reads all PDB files as [biopython Bio.PDB.Structure()](https://biopython.org/wiki/The_Biopython_Structural_Bioinformatics_FAQ) objects at the structures attribute. The attribute is a dictionary whose keys are the protein models names and the values are the Bio.PDB objects. The library can be iterated to get the protein models names at each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6956821",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Structure id=GPX_Bacillus-subtilis>\n",
      "<Structure id=GPX_Gossypium-hirsutum>\n",
      "<Structure id=GPX_Lactococcus-lactis>\n",
      "<Structure id=GPX_Neisseria-meningitidis>\n",
      "<Structure id=GPX_Staphylococcus-aureus>\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(models.structures[model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56220448",
   "metadata": {},
   "source": [
    "## 3. System preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f59b7a",
   "metadata": {},
   "source": [
    "### 3.1 Removing low confidence regions from AlphaFold models at the protein termini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d08dd2",
   "metadata": {},
   "source": [
    "AlphaFold models can contain structural regions with low confidence in their prediction. Since this can represent large structural domains or segments, we are interested in removing them, specially if their are found at the N- and C-termini.\n",
    "\n",
    "The **prepapare_proteins** library has a method to remove terminal segments from AlphaFold structures using the confidence score stored at the B-factor column of the PDBs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "852eb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.removeTerminiByConfidenceScore(confidence_threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80ae9e",
   "metadata": {},
   "source": [
    "The condifedence_threshold keyword indicates the maximum confidence score at which to stop the triming of terminal regions. \n",
    "\n",
    "At any point, when we are modifying our proteins it is a good idea to check that the structural changes have been carried out as we expected. The library has a method for writing all the structures into a folder so we can visualise the state of our set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020192e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.saveModels('trimed_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717353a9",
   "metadata": {},
   "source": [
    "We can open the PDB files with any external programs to check what the previous code did.\n",
    "\n",
    "In the current state of the library, after some modifications on the structures, we need to re-initialise the *proteinModels* class using the models written to a folder with the saveModels() method. For this we simply repeat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e860c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = prepare_proteins.proteinModels('trimed_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029adb0",
   "metadata": {},
   "source": [
    "### 3.2 Align structures to a reference PDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e0865",
   "metadata": {},
   "source": [
    "When comparing related proteins is a good idea to align them to have a common structural framework. The library helps you align the proteins with the method alignModelsToReferencePDB(). We need to give a reference PDB (which, as in this case, can be any PDB from our models), then a folder where to write the aligned structures, and the index (or indedxes) of the chains to align (see the documentation inside the function for details on how the chain_indexes are given). For now we set up the index to be the first folder in the structure (chain_indexes=0) and we run the alignement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b840c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "rescale = 1\n",
      "All-to-all alignment.\n",
      "tbfast-pair (aa) Version 7.490\n",
      "alg=L, model=BLOSUM62, 2.00, -0.10, +0.10, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "rescale = 1\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 2\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     1 /1 \n",
      "done.\n",
      "tbfast (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "rescale = 1\n",
      "\n",
      "    0 / 2\n",
      "Segment   1/  1    1- 159\n",
      "done 001-001-1  identical.   \n",
      "dvtditr (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "rescale = 1\n",
      "All-to-all alignment.\n",
      "tbfast-pair (aa) Version 7.490\n",
      "alg=L, model=BLOSUM62, 2.00, -0.10, +0.10, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "rescale = 1\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 2\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     1 /1 \n",
      "done.\n",
      "tbfast (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "rescale = 1\n",
      "\n",
      "    0 / 2\n",
      "Segment   1/  1    1- 162\n",
      "done 001-001-1  identical.   \n",
      "dvtditr (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "rescale = 1\n",
      "All-to-all alignment.\n",
      "tbfast-pair (aa) Version 7.490\n",
      "alg=L, model=BLOSUM62, 2.00, -0.10, +0.10, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "rescale = 1\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 2\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     1 /1 \n",
      "done.\n",
      "tbfast (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "rescale = 1\n",
      "\n",
      "    0 / 2\n",
      "Segment   1/  1    1- 160\n",
      "done 001-001-1  identical.   \n",
      "dvtditr (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "rescale = 1\n",
      "All-to-all alignment.\n",
      "tbfast-pair (aa) Version 7.490\n",
      "alg=L, model=BLOSUM62, 2.00, -0.10, +0.10, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "rescale = 1\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 2\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     1 /1 \n",
      "done.\n",
      "tbfast (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "rescale = 1\n",
      "\n",
      "    0 / 2\n",
      "Segment   1/  1    1- 179\n",
      "done 001-001-1  identical.   \n",
      "dvtditr (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n",
      "outputhat23=16\n",
      "treein = 0\n",
      "compacttree = 0\n",
      "stacksize: 8192 kb\n",
      "rescale = 1\n",
      "All-to-all alignment.\n",
      "tbfast-pair (aa) Version 7.490\n",
      "alg=L, model=BLOSUM62, 2.00, -0.10, +0.10, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "outputhat23=16\n",
      "Loading 'hat3.seed' ... \n",
      "done.\n",
      "Writing hat3 for iterative refinement\n",
      "rescale = 1\n",
      "Gap Penalty = -1.53, +0.00, +0.00\n",
      "tbutree = 1, compacttree = 0\n",
      "Constructing a UPGMA tree ... \n",
      "    0 / 2\n",
      "done.\n",
      "\n",
      "Progressive alignment ... \n",
      "STEP     1 /1 \n",
      "done.\n",
      "tbfast (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "1 thread(s)\n",
      "\n",
      "minimumweight = 0.000010\n",
      "autosubalignment = 0.000000\n",
      "nthread = 0\n",
      "randomseed = 0\n",
      "blosum 62 / kimura 200\n",
      "poffset = 0\n",
      "niter = 16\n",
      "sueff_global = 0.100000\n",
      "nadd = 16\n",
      "Loading 'hat3' ... done.\n",
      "rescale = 1\n",
      "\n",
      "    0 / 2\n",
      "Segment   1/  1    1- 160\n",
      "done 001-001-1  identical.   \n",
      "dvtditr (aa) Version 7.490\n",
      "alg=A, model=BLOSUM62, 1.53, -0.00, -0.00, noshift, amax=0.0\n",
      "0 thread(s)\n",
      "\n",
      "\n",
      "Strategy:\n",
      " L-INS-i (Probably most accurate, very slow)\n",
      " Iterative refinement method (<16) with LOCAL pairwise alignment information\n",
      "\n",
      "If unsure which option to use, try 'mafft --auto input > output'.\n",
      "For more information, see 'mafft --help', 'mafft --man' and the mafft page.\n",
      "\n",
      "The default gap scoring scheme has been changed in version 7.110 (2013 Oct).\n",
      "It tends to insert more gaps into gap-rich regions than previous versions.\n",
      "To disable this change, add the --leavegappyregion option.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models.alignModelsToReferencePDB('trimed_models/GPX_Bacillus-subtilis.pdb', 'aligned_models', chain_indexes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d0d68",
   "metadata": {},
   "source": [
    "We will continue working with the aligned structures, for that we reload our output models into the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef73d530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/miniconda3/lib/python3.9/site-packages/Bio/PDB/PDBParser.py:395: PDBConstructionWarning: Ignoring unrecognized record 'END' at line 2520\n",
      "  warnings.warn(\n",
      "/home/martin/miniconda3/lib/python3.9/site-packages/Bio/PDB/PDBParser.py:395: PDBConstructionWarning: Ignoring unrecognized record 'END' at line 2507\n",
      "  warnings.warn(\n",
      "/home/martin/miniconda3/lib/python3.9/site-packages/Bio/PDB/PDBParser.py:395: PDBConstructionWarning: Ignoring unrecognized record 'END' at line 2500\n",
      "  warnings.warn(\n",
      "/home/martin/miniconda3/lib/python3.9/site-packages/Bio/PDB/PDBParser.py:395: PDBConstructionWarning: Ignoring unrecognized record 'END' at line 2786\n",
      "  warnings.warn(\n",
      "/home/martin/miniconda3/lib/python3.9/site-packages/Bio/PDB/PDBParser.py:395: PDBConstructionWarning: Ignoring unrecognized record 'END' at line 2507\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "models = prepare_proteins.proteinModels('aligned_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2492d4f",
   "metadata": {},
   "source": [
    "# 4. Prepwizard optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee0463",
   "metadata": {},
   "source": [
    "After our protein models are correctly trimed and aligned we can continue with the prepwizard optimazion of the structures. We create this set up by calling the method setUpPrepwizardOptimization():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f7f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = models.setUpPrepwizardOptimization('prepwizard')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4566277c",
   "metadata": {},
   "source": [
    "Again, the method needs a folder name to put all inout files for the calculations. After executing the method it returns the comamnds to be executed for running the Prepwizard optimization in a machine endowed with the Schrodinger Software license. The commands can be passed to the **bsc_calculations** library to generate the scripts for facilitate the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d9fa099",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsc_calculations.local.parallel(jobs, cpus=min([40, len(jobs)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b04e4b",
   "metadata": {},
   "source": [
    "We define the number of cpus we want to use beforehand, so the library will create one script for each CPU to be used. In our case, we are working with 5 files, so only 5 script files were created. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
